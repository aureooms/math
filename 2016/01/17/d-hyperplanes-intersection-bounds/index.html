<!DOCTYPE html>
<html>
<head>
	<title>&lt;span class=&#34;math&#34;&gt;\(d\)&lt;/span&gt; hyperplanes intersection bounds</title>
	



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css" integrity="sha384-wITovz90syo1dJWVh32uuETPVEtGigN07tkttEqPv+uR2SE/mbQcG7ATL28aI9H0" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js" integrity="sha384-/y1Nn9+QQAipbNQWU65krzJralCnuOasHncUFXGkdwntGeSvQicrYkiUBwsgUqc1" crossorigin="anonymous"></script>
<script>
document.addEventListener("DOMContentLoaded", function ( ) {

	document.querySelectorAll( '.math' )
	.forEach( function ( element ) {
		try {
			var tex = element.textContent ;
			var math = tex.substr(2, tex.length - 4) ;
			var displayMode = tex.substr(0, 2) == '\\[' ;
			try {
				katex.render(math, element, {displayMode: displayMode});
			}
			catch ( e ) {
				console.error( e ) ;
				console.debug( element, tex , math , { displayMode: displayMode});
			}
		}
		catch ( e ) {
			console.error( e ) ;
		}
	});

});
</script>

</head>
<body>


  <h1><span class="math">\(d\)</span> hyperplanes intersection bounds</h1>
  <h2>Sun, Jan 17, 2016</h2>
  <p>We bound the position of the <span  class="math">\(0\)</span>-cells of an arrangement of hyperplanes in
<span  class="math">\(\mathbb{R}^d\)</span>. This allows, for example, to build an hypercube that
intersects all cells of the arrangement. Such an hypercube must contain at
least one point of each cell of the arrangement. When <span  class="math">\(q > 0\)</span>, in order to
fix which point of a <span  class="math">\(q\)</span>-cell we want to include in the hypercube, it
suffices to add the <span  class="math">\(n\)</span> hyperplanes of equation <span  class="math">\(x_i = 0\)</span> to the
arrangement. With those additional hyperplanes, the arrangement is such that
each <span  class="math">\(q\)</span>-cell of the arrangement with <span  class="math">\(q > 0\)</span> contains a
<span  class="math">\(0\)</span>-cell of the arrangement, hence, we only need the hypercube to
intersect, for each <span  class="math">\(0\)</span>-cell <span  class="math">\(\nu\)</span> of the arrangement, an hypersphere
of center <span  class="math">\(\nu\)</span> and arbitrarily small radius. The inequalities of the
polyhedral set defining our hypercube will thus only depend on the position of
the <span  class="math">\(0\)</span>-cells of our arrangement.</p>

<p>
We thus bound the components of those vertices. Since the <span  class="math">\(0\)</span>-cells of our
arrangement are intersections of <span  class="math">\(n\)</span> intersecting and linearly independent
hyperplanes, we focus on the solutions of systems of <span  class="math">\(n\)</span> linear equations
in <span  class="math">\(\mathbb{R}^n\)</span>.
Let us begin with a few examples to build some intuition.</p>

<h2 id="examples">Examples</h2>

<h3 id="first-example-a-x--b--0">First example: <span  class="math">\(a x + b = 0\)</span></h3>

<p>If <span  class="math">\(a \neq 0\)</span> then</p>

<p>$$</p>

<pre><code>a x + b = 0 \iff x = -\frac{b}{a}.
</code></pre>

<p>$$</p>

<h3 id="second-example-a-x--b-y--c--0">Second example: <span  class="math">\(a x + b y + c = 0\)</span></h3>

<p>If <span  class="math">\(
\begin{vmatrix}
a & b \\
d & e
\end{vmatrix} \neq 0 \)</span>
then to solve</p>

<p><span  class="math">\[
\left\lbrace\begin{aligned}
a x + b y + c &= 0\\
d x + e y + f &= 0
\end{aligned}\right.,
\]</span></p>

<p>we can use the following (implied) equations which are true for all
<span  class="math">\(\lambda,\mu \in \mathbb{R}\)</span></p>

<p>$$
\begin{aligned}</p>

<pre><code>(a+\lambda d) x + (b + \lambda e ) y + ( c + \lambda f ) &amp;= 0\\
(\mu a+ d) x + (\mu b +  e ) y + ( \mu c + f ) &amp;= 0.
</code></pre>

<p>\end{aligned}
$$</p>

<p>We can find <span  class="math">\(x\)</span> by making <span  class="math">\(y\)</span> disappear. Since <span  class="math">\(
\begin{vmatrix}
a & b \\
d & e
\end{vmatrix} \neq 0 \)</span> we cannot have <span  class="math">\(b\)</span> and <span  class="math">\(e\)</span> equal to zero
simultaneously so either <span  class="math">\(e \neq 0\)</span> and</p>

<p><span  class="math">\[
\begin{aligned}
\lambda = -\frac{b}{e} \implies
(a - \frac{b}{e} d) x +
(b - \frac{b}{e} e) y +
(c - \frac{b}{e} f) & = 0\\
(ae - bd) x +
(be - be) y +
(ce - bf) & = 0,
\end{aligned}
\]</span></p>

<p>or <span  class="math">\(b \neq 0\)</span> and</p>

<p><span  class="math">\[
\begin{aligned}
\mu = -\frac{e}{b} \implies
(-\frac{e}{b}a + d) x +
(-\frac{e}{b}b + e) y +
(-\frac{e}{b}c + f) & = 0\\
(ae - bd) x +
(be - be) y +
(ce - bf) & = 0,
\end{aligned}
\]</span></p>

<p>hence, in either case</p>

<p><span  class="math">\[
x = -\frac{ce - bf}{ae-bd}
= +\frac{
\begin{vmatrix}
b & c \\
e & f
\end{vmatrix}
}{
\begin{vmatrix}
a & b \\
d & e
\end{vmatrix}
}.
\]</span></p>

<p>Similarily, we can find <span  class="math">\(y\)</span> by making <span  class="math">\(x\)</span> disappear. Since <span  class="math">\(
\begin{vmatrix}
a & b \\
d & e
\end{vmatrix} \neq 0 \)</span> we cannot have <span  class="math">\(a\)</span> and <span  class="math">\(d\)</span> equal to zero
simultaneously so either <span  class="math">\(d \neq 0\)</span> and</p>

<p><span  class="math">\[
\begin{aligned}
\lambda = -\frac{a}{d} \implies
(a - \frac{a}{d} d) x +
(b - \frac{a}{d} e) y +
(c - \frac{a}{d} f) & = 0\\
(ad - ad) x +
(bd - ae) y +
(cd - af) & = 0,
\end{aligned}
\]</span></p>

<p>or <span  class="math">\(a \neq 0\)</span> and</p>

<p><span  class="math">\[
\begin{aligned}
\mu = -\frac{d}{a} \implies
(-\frac{d}{a}a + d) x +
(-\frac{d}{a}b + e) y +
(-\frac{d}{a}c + f) & = 0\\
(ad - ad) x +
(bd - ae) y +
(cd - af) & = 0,
\end{aligned}
\]</span></p>

<p>hence, in either case</p>

<p><span  class="math">\[
y = -\frac{cd - af}{bd-ae}
= -\frac{
\begin{vmatrix}
a & c \\
d & f
\end{vmatrix}
}{
\begin{vmatrix}
a & b \\
d & e
\end{vmatrix}
}.
\]</span></p>

<h2 id="same-examples-using-a-better-notation">Same examples, using a better notation</h2>

<p>Let us do it again with a better notation.</p>

<h3 id="first-example-alpha10--alpha11-x1--0">First example: <span  class="math">\(\alpha_{1,0} + \alpha_{1,1} x_1 = 0\)</span></h3>

<p>If <span  class="math">\(\alpha_{1,1} \neq 0\)</span> then</p>

<p>$$</p>

<pre><code>\alpha_{1,0} + \alpha_{1,1} x_1 = 0 \iff x_1 = -\frac{\alpha_{1,0}}{\alpha_{1,1}}.
</code></pre>

<p>$$</p>

<h3 id="second-example-alpha10--alpha11-x1--alpha12-x2--0">Second example: <span  class="math">\(\alpha_{1,0} + \alpha_{1,1} x_1 + \alpha_{1,2} x_2 = 0\)</span></h3>

<p>If <span  class="math">\(
\begin{vmatrix}
\alpha_{1,1} & \alpha_{1,2} \\
\alpha_{2,1} & \alpha_{2,2}
\end{vmatrix} \neq 0 \)</span>
then to solve</p>

<p><span  class="math">\[
\left\lbrace\begin{aligned}
\alpha_{1,0} + \alpha_{1,1} x_1 + \alpha_{1,2} x_2 &= 0\\
\alpha_{2,0} + \alpha_{2,1} x_1 + \alpha_{2,2} x_2 &= 0
\end{aligned}\right.,
\]</span></p>

<p>we can use the following (implied) equations which are true for all
<span  class="math">\(y_1,y_2 \in \mathbb{R}\)</span></p>

<p>$$
\begin{aligned}</p>

<pre><code>(\alpha_{1,0}+ \alpha_{2,0}y_2 ) +
(\alpha_{1,1}+ \alpha_{2,1}y_2 ) x_1 +
(\alpha_{1,2}+ \alpha_{2,2}y_2 ) x_2 = 0\\
(\alpha_{1,0}y_1+ \alpha_{2,0} ) +
(\alpha_{1,1}y_1+ \alpha_{2,1} ) x_1 +
(\alpha_{1,2}y_1+ \alpha_{2,2} ) x_2 = 0.
</code></pre>

<p>\end{aligned}
$$</p>

<p>We can find <span  class="math">\(x_1\)</span> by making <span  class="math">\(x_2\)</span> disappear. Since <span  class="math">\(
\begin{vmatrix}
\alpha_{1,1} & \alpha_{1,2} \\
\alpha_{2,1} & \alpha_{2,2}
\end{vmatrix} \neq 0 \)</span> we cannot have <span  class="math">\(\alpha_{1,2}\)</span> and <span  class="math">\(\alpha_{2,2}\)</span> equal to zero
simultaneously so either <span  class="math">\(\alpha_{2,2} \neq 0\)</span> and</p>

<p><span  class="math">\[
\begin{aligned}
y_2 = -\frac{\alpha_{1,2}}{\alpha_{2,2}} \implies
(\alpha_{1,0} - \frac{\alpha_{1,2}}{\alpha_{2,2}} \alpha_{2,0}) +
(\alpha_{1,1} - \frac{\alpha_{1,2}}{\alpha_{2,2}} \alpha_{2,1}) x_1 +
(\alpha_{1,2} - \frac{\alpha_{1,2}}{\alpha_{2,2}} \alpha_{2,2}) x_2
& = 0\\
(\alpha_{1,0}\alpha_{2,2} - \alpha_{1,2}\alpha_{2,0}) +
(\alpha_{1,1}\alpha_{2,2} - \alpha_{1,2}\alpha_{2,1}) x_1 +
(\alpha_{1,2}\alpha_{2,2} - \alpha_{1,2}\alpha_{2,2}) x_2 & = 0
\end{aligned}
\]</span></p>

<p>or <span  class="math">\(\alpha_{1,2} \neq 0\)</span> and</p>

<p><span  class="math">\[
\begin{aligned}
y_1 = -\frac{\alpha_{2,2}}{\alpha_{1,2}} \implies
(-\frac{\alpha_{2,2}}{\alpha_{1,2}}\alpha_{1,0} + \alpha_{2,0}) +
(-\frac{\alpha_{2,2}}{\alpha_{1,2}}\alpha_{1,1} + \alpha_{2,1}) x_1 +
(-\frac{\alpha_{2,2}}{\alpha_{1,2}}\alpha_{1,2} + \alpha_{2,2}) x_2
& = 0\\
(\alpha_{1,0}\alpha_{2,2} - \alpha_{1,2}\alpha_{2,0}) +
(\alpha_{1,1}\alpha_{2,2} - \alpha_{1,2}\alpha_{2,1}) x_1 +
(\alpha_{1,2}\alpha_{2,2} - \alpha_{1,2}\alpha_{2,2}) x_2 & = 0
\end{aligned}
\]</span></p>

<p>hence, in either case</p>

<p><span  class="math">\[
x_1 = -\frac{\alpha_{1,0}\alpha_{2,2} - \alpha_{1,2}\alpha_{2,0}}
{\alpha_{1,1}\alpha_{2,2} - \alpha_{1,2}\alpha_{2,1}}
= -\frac{
\begin{vmatrix}
\alpha_{1,0} & \alpha_{1,2} \\
\alpha_{2,0} & \alpha_{2,2}
\end{vmatrix}
}{
\begin{vmatrix}
\alpha_{1,1} & \alpha_{1,2} \\
\alpha_{2,1} & \alpha_{2,2}
\end{vmatrix}
}.
\]</span></p>

<p>Similarily, we can find <span  class="math">\(x_2\)</span> by making <span  class="math">\(x_1\)</span> disappear. Since <span  class="math">\(
\begin{vmatrix}
\alpha_{1,1} & \alpha_{1,2} \\
\alpha_{2,1} & \alpha_{2,2}
\end{vmatrix} \neq 0 \)</span> we cannot have <span  class="math">\(\alpha_{1,1}\)</span> and <span  class="math">\(\alpha_{2,1}\)</span> equal to zero
simultaneously so either <span  class="math">\(\alpha_{2,1} \neq 0\)</span> and</p>

<p><span  class="math">\[
\begin{aligned}
y_2 = -\frac{\alpha_{1,1}}{\alpha_{2,1}} \implies
(\alpha_{1,0} - \frac{\alpha_{1,1}}{\alpha_{2,1}} \alpha_{2,0}) +
(\alpha_{1,1} - \frac{\alpha_{1,1}}{\alpha_{2,1}} \alpha_{2,1}) x_1 +
(\alpha_{1,2} - \frac{\alpha_{1,1}}{\alpha_{2,1}} \alpha_{2,2}) x_2
& = 0\\
(\alpha_{1,0}\alpha_{2,1} - \alpha_{1,1}\alpha_{2,0}) +
(\alpha_{1,1}\alpha_{2,1} - \alpha_{1,1}\alpha_{2,1}) x_1 +
(\alpha_{1,2}\alpha_{2,1} - \alpha_{1,1}\alpha_{2,2}) x_2 & = 0
\end{aligned}
\]</span></p>

<p>or <span  class="math">\(\alpha_{1,1} \neq 0\)</span> and</p>

<p><span  class="math">\[
\begin{aligned}
y_1 = -\frac{\alpha_{2,1}}{\alpha_{1,1}} \implies
(-\frac{\alpha_{2,1}}{\alpha_{1,1}}\alpha_{1,0} + \alpha_{2,0}) +
(-\frac{\alpha_{2,1}}{\alpha_{1,1}}\alpha_{1,1} + \alpha_{2,1}) x_1 +
(-\frac{\alpha_{2,1}}{\alpha_{1,1}}\alpha_{1,2} + \alpha_{2,2}) x_2
& = 0\\
(\alpha_{1,0}\alpha_{2,1} - \alpha_{1,1}\alpha_{2,0}) +
(\alpha_{1,1}\alpha_{2,1} - \alpha_{1,1}\alpha_{2,1}) x_1 +
(\alpha_{1,2}\alpha_{2,1} - \alpha_{1,1}\alpha_{2,2}) x_2 & = 0
\end{aligned}
\]</span></p>

<p>hence, in either case</p>

<p><span  class="math">\[
x_2 = -\frac{\alpha_{1,0}\alpha_{2,1} - \alpha_{1,1}\alpha_{2,0}}
{\alpha_{1,2}\alpha_{2,1} - \alpha_{1,1}\alpha_{2,2}}
= +\frac{
\begin{vmatrix}
\alpha_{1,0} & \alpha_{1,1} \\
\alpha_{2,0} & \alpha_{2,1}
\end{vmatrix}
}{
\begin{vmatrix}
\alpha_{1,1} & \alpha_{1,2} \\
\alpha_{2,1} & \alpha_{2,2}
\end{vmatrix}
}.
\]</span></p>

<h2 id="the-general-case">The general case</h2>

<h3 id="theorem">Theorem</h3>

<p>For all <span  class="math">\(n > 0 \in \mathbb{N}\)</span>, if</p>

<p><span  class="math">\[
\begin{vmatrix}
\alpha_{1,1} & \alpha_{1,2} & \cdots & \alpha_{1,n}\\
\alpha_{2,1} & \alpha_{2,2} & \cdots & \alpha_{2,n}\\
\vdots & \vdots & \ddots & \vdots\\
\alpha_{n,1} & \alpha_{n,2} & \cdots & \alpha_{n,n}
\end{vmatrix} \neq 0,
\]</span></p>

<p>where <span  class="math">\(\alpha_{i,j} \in \mathbb{R}\)</span> for all <span  class="math">\((i,j) \in [n] \times
\{\,0,1,\ldots,n\,\}\)</span>,
then the system of linear equations</p>

<p><span  class="math">\[
\left\lbrace\begin{array}{cccccccccccc}
\alpha_{1,0} &+& \alpha_{1,1} x_1 &+& \alpha_{1,2} x_2 &+& \cdots &+& \alpha_{1,n} x_n &=& 0\\
\alpha_{2,0} &+& \alpha_{2,1} x_1 &+& \alpha_{2,2} x_2 &+& \cdots &+& \alpha_{2,n} x_n &=& 0\\
\vdots &+& \vdots &+& \vdots &+& \ddots &+& \vdots &=& \vdots\\
\alpha_{n,0} &+& \alpha_{n,1} x_1 &+& \alpha_{n,2} x_2 &+& \cdots &+& \alpha_{n,n} x_n &=& 0
\end{array}\right.
\]</span></p>

<p>has a unique solution <span  class="math">\(x=(x_1,x_2,\ldots,x_n)\)</span> such that for all <span  class="math">\(i \in [n]\)</span></p>

<p><span  class="math">\[
x_i = (-1)^i
\frac{
\begin{vmatrix}
\alpha_{1,0} & \alpha_{1,1} & \alpha_{1,2} & \cdots & \alpha_{1,i-1} & \alpha_{1,i+1} & \cdots & \alpha_{1,n}\\
\alpha_{2,0} & \alpha_{2,1} & \alpha_{2,2} & \cdots & \alpha_{2,i-1} & \alpha_{2,i+1} & \cdots & \alpha_{2,n}\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
\alpha_{n,0} & \alpha_{n,1} & \alpha_{n,2} & \cdots & \alpha_{n,i-1} & \alpha_{n,i+1} & \cdots & \alpha_{n,n}
\end{vmatrix}
}
{
\begin{vmatrix}
\alpha_{1,1} & \alpha_{1,2} & \cdots & \alpha_{1,n}\\
\alpha_{2,1} & \alpha_{2,2} & \cdots & \alpha_{2,n}\\
\vdots & \vdots & \ddots & \vdots\\
\alpha_{n,1} & \alpha_{n,2} & \cdots & \alpha_{n,n}
\end{vmatrix}
}
\]</span></p>

<h3 id="proof">Proof</h3>

<p>By induction on <span  class="math">\(n\)</span>. We reduce the problem of finding <span  class="math">\(x_i\)</span> to the
problem of finding a solution to a system of <span  class="math">\(n-1\)</span> linear equations with
<span  class="math">\(n-1\)</span> unknowns.</p>

<h4 id="base-case">Base case</h4>

<p>We already showed that it holds for <span  class="math">\(n=1\)</span> and <span  class="math">\(n=2\)</span> in the examples
given previously.</p>

<h4 id="induction">Induction</h4>

<p>Suppose that the theorem holds up to <span  class="math">\(n-1\)</span> (this is the induction
hypothesis), we show that it must also hold for <span  class="math">\(n\)</span>.</p>

<p>Fix <span  class="math">\(i \in [n]\)</span>, the system of equations</p>

<p><span  class="math">\[
\left\lbrace\begin{array}{cccccccccccc}
\alpha_{1,0} &+& \alpha_{1,1} x_1 &+& \alpha_{1,2} x_2 &+& \cdots &+& \alpha_{1,n} x_n &=& 0\\
\alpha_{2,0} &+& \alpha_{2,1} x_1 &+& \alpha_{2,2} x_2 &+& \cdots &+& \alpha_{2,n} x_n &=& 0\\
\vdots &+& \vdots &+& \vdots &+& \ddots &+& \vdots &=& \vdots\\
\alpha_{n,0} &+& \alpha_{n,1} x_1 &+& \alpha_{n,2} x_2 &+& \cdots &+& \alpha_{n,n} x_n &=& 0
\end{array}\right.
\]</span></p>

<p>implies for all <span  class="math">\(y_1,y_2,\ldots,y_n \in \mathbb{R}\)</span> that</p>

<p><span  class="math">\[
\begin{array}{crccccccccccclccc}
 &(&\alpha_{1,0} y_1 &+& \alpha_{2,0} y_2 &+& \alpha_{3,0} y_3 &+& \cdots &+& \alpha_{n,0} y_n &)&&&\\
+&(&\alpha_{1,1} y_1 &+& \alpha_{2,1} y_2 &+& \alpha_{3,1} y_3 &+& \cdots &+& \alpha_{n,1} y_n &)&x_1&&\\
+&(&\alpha_{1,2} y_1 &+& \alpha_{2,2} y_2 &+& \alpha_{3,2} y_3 &+& \cdots &+& \alpha_{n,2} y_n &)&x_2&&\\
+&(&\vdots             &+& \vdots             &+& \vdots             &+& \ddots &+& \vdots             &)&\vdots&&\\
+&(&\alpha_{1,n} y_1 &+& \alpha_{2,n} y_2 &+& \alpha_{3,n} y_3 &+& \cdots &+& \alpha_{n,n} y_n &)&x_n&=& 0.
\end{array}
\]</span></p>

<p>Note that we can fix one of the <span  class="math">\(y_j\)</span> to some arbitrary constant while the
equation remains valid.</p>

<p>In order to determine <span  class="math">\(x_i\)</span> it suffices to make
the <span  class="math">\(x_k \neq x_i\)</span> disappear. Let us choose <span  class="math">\(\ell \in [n]\)</span> such that</p>

<p><span  class="math">\[
\begin{vmatrix}
\alpha_{1,1}   & \alpha_{2,1}   & \cdots & \alpha_{\ell-1,1} & \alpha_{\ell+1,1}   & \cdots & \alpha_{n,1}\\
\alpha_{1,2}   & \alpha_{2,2}   & \cdots & \alpha_{\ell-1,2} & \alpha_{\ell+1,2}   & \cdots & \alpha_{n,2}\\
\vdots          & \vdots          & \ddots & \vdots          & \vdots            & \ddots & \vdots\\
\alpha_{1,i-1} & \alpha_{2,i-1} & \cdots & \alpha_{\ell-1,i-1} & \alpha_{\ell+1,i-1} & \cdots & \alpha_{n,i-1}\\
\alpha_{1,i+1} & \alpha_{2,i+1} & \cdots & \alpha_{\ell-1,i+1} & \alpha_{\ell+1,i+1} & \cdots & \alpha_{n,i+1}\\
\vdots          & \vdots          & \ddots & \vdots          & \vdots            & \ddots & \vdots\\
\alpha_{1,n}   & \alpha_{2,n}   & \cdots & \alpha_{\ell-1,n} & \alpha_{\ell+1,n}   & \cdots & \alpha_{n,n}
\end{vmatrix} \neq 0.
\]</span></p>

<p>Such a <span  class="math">\(\ell\)</span> always exists as otherwise</p>

<p><span  class="math">\[
\begin{vmatrix}
\alpha_{1,1} & \alpha_{1,2} & \cdots & \alpha_{1,n}\\
\alpha_{2,1} & \alpha_{2,2} & \cdots & \alpha_{2,n}\\
\vdots & \vdots & \ddots & \vdots\\
\alpha_{n,1} & \alpha_{n,2} & \cdots & \alpha_{n,n}
\end{vmatrix} = 0.
\]</span></p>

<p>Without loss of generality we could assume that <span  class="math">\(\ell=1\)</span>. However, for the sake
of explanation, we will introduce a new notation to handle all cases directly.</p>

<p>We let <span  class="math">\(\beta_{a,b} = \alpha_{a,b}\)</span> for all <span  class="math">\(a \in \{\,2,\ldots,\ell-1,\ell+1,\ldots,n\,\}\)</span> and all
<span  class="math">\(b \in \{\,0,1,\ldots,n\,\}\)</span>. We let <span  class="math">\(\beta_{1,b} =
\alpha_{\ell,b}\)</span> and <span  class="math">\(\beta_{\ell,b} = \alpha_{1,b}\)</span> for all <span  class="math">\(b \in \{\,0,1,\ldots,n\,\}\)</span>.
For all
<span  class="math">\(a \in \{\,0,2,\ldots,\ell-1,\ell+1,\ldots,n\,\}\)</span> we let <span  class="math">\(z_a =
y_a\)</span>. We let <span  class="math">\(z_1 = y_\ell\)</span> and <span  class="math">\(z_\ell = y_1\)</span>.</p>

<p>If we fix <span  class="math">\(z_1 = 1\)</span>, our equation remains valid and we obtain the
following system of <span  class="math">\(n-1\)</span> linear equations in <span  class="math">\(n-1\)</span> unknowns</p>

<p><span  class="math">\[
\left\lbrace\begin{array}{cccccccccccc}
\beta_{1,1} &+& \beta_{2,1} z_2 &+& \beta_{3,1} z_3 &+& \cdots &+& \beta_{n,1} z_n &=& 0\\
\beta_{1,2} &+& \beta_{2,2} z_2 &+& \beta_{3,2} z_3 &+& \cdots &+& \beta_{n,2} z_n &=& 0\\
\vdots &+& \vdots &+& \vdots &+& \ddots &+& \vdots &=& \vdots\\
\beta_{1,i-1} &+& \beta_{2,i-1} z_2 &+& \beta_{3,i-1} z_3 &+& \cdots &+& \beta_{n,i-1} z_n &=& 0\\
\beta_{1,i+1} &+& \beta_{2,i+1} z_2 &+& \beta_{3,i+1} z_3 &+& \cdots &+& \beta_{n,i+1} z_n &=& 0\\
\vdots &+& \vdots &+& \vdots &+& \ddots &+& \vdots &=& \vdots\\
\beta_{1,n} &+& \beta_{2,n} z_2 &+& \beta_{3,n} z_3 &+& \cdots &+& \beta_{n,n} z_n &=& 0
\end{array}\right..
\]</span></p>

<p>By our choice of <span  class="math">\(\ell\)</span> we have that</p>

<p><span  class="math">\[
\begin{vmatrix}
\beta_{\ell,1}   & \beta_{2,1}   & \cdots & \beta_{\ell-1,1}   & \beta_{\ell+1,1}   & \cdots & \beta_{n,1}\\
\beta_{\ell,2}   & \beta_{2,2}   & \cdots & \beta_{\ell-1,2}   & \beta_{\ell+1,2}   & \cdots & \beta_{n,2}\\
\vdots             & \vdots         & \ddots & \vdots               & \vdots               & \ddots & \vdots\\
\beta_{\ell,i-1} & \beta_{2,i-1} & \cdots & \beta_{\ell-1,i-1} & \beta_{\ell+1,i-1} & \cdots & \beta_{n,i-1}\\
\beta_{\ell,i+1} & \beta_{2,i+1} & \cdots & \beta_{\ell-1,i+1} & \beta_{\ell+1,i+1} & \cdots & \beta_{n,i+1}\\
\vdots             & \vdots         & \ddots & \vdots               & \vdots               & \ddots & \vdots\\
\beta_{\ell,n}   & \beta_{2,n}   & \cdots & \beta_{\ell-1,n}   & \beta_{\ell+1,n}   & \cdots & \beta_{n,n}
\end{vmatrix} \neq 0,
\]</span></p>

<p>hence, by moving the first column to the right place,</p>

<p><span  class="math">\[
\begin{vmatrix}
\beta_{2,1}   & \beta_{3,1}   & \cdots & \beta_{n,1}\\
\beta_{2,2}   & \beta_{3,2}   & \cdots & \beta_{n,2}\\
\vdots          & \vdots          & \ddots & \vdots\\
\beta_{2,i-1} & \beta_{3,i-1} & \cdots & \beta_{n,i-1}\\
\beta_{2,i+1} & \beta_{3,i+1} & \cdots & \beta_{n,i+1}\\
\vdots          & \vdots          & \ddots & \vdots\\
\beta_{2,n}   & \beta_{3,n}   & \cdots & \beta_{n,n}
\end{vmatrix} \neq 0.
\]</span></p>

<p>Thus, by the induction hypothesis, our last system of equations
has a unique solution <span  class="math">\(z=(z_2,z_3,\ldots,z_n)\)</span> such that for all <span  class="math">\(j
\in [2,n]\)</span></p>

<p><span  class="math">\[
z_j = (-1)^{j+1}
\frac{
\begin{vmatrix}
\beta_{1,1}   & \beta_{2,1}   & \beta_{3,1}   & \cdots & \beta_{j-1,1} & \beta_{j+1,1}  & \cdots & \beta_{n,1}\\
\beta_{1,2}   & \beta_{2,2}   & \beta_{3,2}   & \cdots & \beta_{j-1,2} & \beta_{j+1,2}  & \cdots & \beta_{n,2}\\
\vdots          & \vdots          & \vdots          & \ddots & \vdots & \vdots  & \ddots & \vdots\\
\beta_{1,i-1} & \beta_{2,i-1} & \beta_{3,i-1} & \cdots & \beta_{j-1,i-1} & \beta_{j+1,i-1}  & \cdots & \beta_{n,i-1}\\
\beta_{1,i+1} & \beta_{2,i+1} & \beta_{3,i+1} & \cdots & \beta_{j-1,i+1} & \beta_{j+1,i+1}  & \cdots & \beta_{n,i+1}\\
\vdots          & \vdots          & \vdots          & \ddots & \vdots & \vdots  & \ddots & \vdots\\
\beta_{1,n}   & \beta_{2,n}   & \beta_{3,n}   & \cdots & \beta_{j-1,n} & \beta_{j+1,n}  & \cdots & \beta_{n,n}
\end{vmatrix}
}
{
\begin{vmatrix}
\beta_{2,1}   & \beta_{3,1}   & \cdots & \beta_{n,1}\\
\beta_{2,2}   & \beta_{3,2}   & \cdots & \beta_{n,2}\\
\vdots          & \vdots          & \ddots & \vdots\\
\beta_{2,i-1} & \beta_{3,i-1} & \cdots & \beta_{n,i-1}\\
\beta_{2,i+1} & \beta_{3,i+1} & \cdots & \beta_{n,i+1}\\
\vdots          & \vdots          & \ddots & \vdots\\
\beta_{2,n}   & \beta_{3,n}   & \cdots & \beta_{n,n}
\end{vmatrix}
}.
\]</span></p>

<p>Now that the <span  class="math">\(x_k \neq x_i\)</span> disappeared, we have</p>

<p><span  class="math">\[
\begin{array}{crccccccccccclccc}
&(&\beta_{1,0} &+& \beta_{2,0} z_2 &+& \beta_{3,0} z_3 &+& \cdots &+& \beta_{n,0} z_n &)&&&\\
+&(&\beta_{1,i} &+& \beta_{2,i} z_2 &+& \beta_{3,i} z_3 &+& \cdots &+& \beta_{n,i} z_n &)&x_i&=& 0,
\end{array}
\]</span></p>

<p>hence,</p>

<p><span  class="math">\[
x_i = -\frac{
\beta_{1,0} + \beta_{2,0} z_2 + \beta_{3,0} z_3 + \cdots + \beta_{n,0} z_n
}
{
\beta_{1,i} + \beta_{2,i} z_2 + \beta_{3,i} z_3 + \cdots + \beta_{n,i} z_n
},
\]</span></p>

<p>that is, by replacing the <span  class="math">\(z_j\)</span> by their respective values,</p>

<p><span  class="math">\[
x_i = -\frac{
\begin{vmatrix}
\beta_{1,0}   & \beta_{2,0}   & \beta_{3,0}   & \cdots & \beta_{n,0}\\
\beta_{1,1}   & \beta_{2,1}   & \beta_{3,1}   & \cdots & \beta_{n,1}\\
\beta_{1,2}   & \beta_{2,2}   & \beta_{3,2}   & \cdots & \beta_{n,2}\\
\vdots         & \vdots         & \vdots         & \ddots & \vdots\\
\beta_{1,i-1} & \beta_{2,i-1} & \beta_{3,i-1} & \cdots & \beta_{n,i-1}\\
\beta_{1,i+1} & \beta_{2,i+1} & \beta_{3,i+1} & \cdots & \beta_{n,i+1}\\
\vdots         & \vdots         & \vdots         & \ddots & \vdots\\
\beta_{1,n}   & \beta_{2,n}   & \beta_{3,n}   & \cdots & \beta_{n,n}
\end{vmatrix}
}
{
\begin{vmatrix}
\beta_{1,i}   & \beta_{2,i}   & \beta_{3,i}   & \cdots & \beta_{n,i}\\
\beta_{1,2}   & \beta_{2,2}   & \beta_{3,2}   & \cdots & \beta_{n,2}\\
\vdots         & \vdots         & \vdots         & \ddots & \vdots\\
\beta_{1,i-1} & \beta_{2,i-1} & \beta_{3,i-1} & \cdots & \beta_{n,i-1}\\
\beta_{1,i+1} & \beta_{2,i+1} & \beta_{3,i+1} & \cdots & \beta_{n,i+1}\\
\vdots         & \vdots         & \vdots         & \ddots & \vdots\\
\beta_{1,n}   & \beta_{2,n}   & \beta_{3,n}   & \cdots & \beta_{n,n}
\end{vmatrix}
},
\]</span></p>

<p>which we can rearrange by swapping rows of the second determinant to get</p>

<p><span  class="math">\[
x_i = -\frac{
\begin{vmatrix}
\beta_{1,0}   & \beta_{2,0}   & \beta_{3,0}   & \cdots & \beta_{n,0}\\
\beta_{1,1}   & \beta_{2,1}   & \beta_{3,1}   & \cdots & \beta_{n,1}\\
\beta_{1,2}   & \beta_{2,2}   & \beta_{3,2}   & \cdots & \beta_{n,2}\\
\vdots         & \vdots         & \vdots         & \ddots & \vdots\\
\beta_{1,i-1} & \beta_{2,i-1} & \beta_{3,i-1} & \cdots & \beta_{n,i-1}\\
\beta_{1,i+1} & \beta_{2,i+1} & \beta_{3,i+1} & \cdots & \beta_{n,i+1}\\
\vdots         & \vdots         & \vdots         & \ddots & \vdots\\
\beta_{1,n}   & \beta_{2,n}   & \beta_{3,n}   & \cdots & \beta_{n,n}
\end{vmatrix}
}
{
(-1)^{i-1}
\begin{vmatrix}
\beta_{1,2}   & \beta_{2,2}   & \beta_{3,2}   & \cdots & \beta_{n,2}\\
\vdots         & \vdots         & \vdots         & \ddots & \vdots\\
\beta_{1,i-1} & \beta_{2,i-1} & \beta_{3,i-1} & \cdots & \beta_{n,i-1}\\
\beta_{1,i}   & \beta_{2,i}   & \beta_{3,i}   & \cdots & \beta_{n,i}\\
\beta_{1,i+1} & \beta_{2,i+1} & \beta_{3,i+1} & \cdots & \beta_{n,i+1}\\
\vdots         & \vdots         & \vdots         & \ddots & \vdots\\
\beta_{1,n}   & \beta_{2,n}   & \beta_{3,n}   & \cdots & \beta_{n,n}
\end{vmatrix}
},
\]</span></p>

<p>that is, by the fact that <span  class="math">\(\mathop{det}(M) = \mathop{det}(M^T)\)</span>,</p>

<p><span  class="math">\[
x_i = (-1)^i
\frac{
\begin{vmatrix}
\beta_{1,0} & \beta_{1,1} & \beta_{1,2} & \cdots & \beta_{1,i-1} & \beta_{1,i+1} & \cdots & \beta_{1,n}\\
\beta_{2,0} & \beta_{2,1} & \beta_{2,2} & \cdots & \beta_{2,i-1} & \beta_{2,i+1} & \cdots & \beta_{2,n}\\
\vdots       & \vdots       & \vdots       & \ddots & \vdots         & \vdots         & \ddots & \vdots\\
\beta_{n,0} & \beta_{n,1} & \beta_{n,2} & \cdots & \beta_{n,i-1} & \beta_{n,i+1} & \cdots & \beta_{n,n}
\end{vmatrix}
}
{
\begin{vmatrix}
\beta_{1,1} & \beta_{1,2} & \cdots & \beta_{1,i-1} & \beta_{1,i} & \beta_{1,i+1} & \cdots & \beta_{1,n}\\
\beta_{2,1} & \beta_{2,2} & \cdots & \beta_{2,i-1} & \beta_{2,i} & \beta_{2,i+1} & \cdots & \beta_{2,n}\\
\vdots       & \vdots       & \ddots & \vdots         & \vdots       & \vdots         & \ddots & \vdots\\
\beta_{n,1} & \beta_{n,2} & \cdots & \beta_{n,i-1} & \beta_{n,i} & \beta_{n,i+1} & \cdots & \beta_{n,n}
\end{vmatrix}
}.
\]</span></p>

<p>It remains to trade <span  class="math">\(\beta\)</span> for <span  class="math">\(\alpha\)</span> according to the bijection
we defined earlier, then swap the first column with the <span  class="math">\(\ell^{\text{th}}\)</span>
one if necessary.</p>

<p>If <span  class="math">\(\ell = 1\)</span>, we have nothing to do. Otherwise swaping the columns
multiplies both determinants by <span  class="math">\(-1\)</span>, hence the sign of
the ratio remains unchanged. We get</p>

<p><span  class="math">\[
x_i = (-1)^i
\frac{
\begin{vmatrix}
\alpha_{1,0} & \alpha_{1,1} & \alpha_{1,2} & \cdots & \alpha_{1,i-1} & \alpha_{1,i+1} & \cdots & \alpha_{1,n}\\
\alpha_{2,0} & \alpha_{2,1} & \alpha_{2,2} & \cdots & \alpha_{2,i-1} & \alpha_{2,i+1} & \cdots & \alpha_{2,n}\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
\alpha_{n,0} & \alpha_{n,1} & \alpha_{n,2} & \cdots & \alpha_{n,i-1} & \alpha_{n,i+1} & \cdots & \alpha_{n,n}
\end{vmatrix}
}
{
\begin{vmatrix}
\alpha_{1,1} & \alpha_{1,2} & \cdots & \alpha_{1,n}\\
\alpha_{2,1} & \alpha_{2,2} & \cdots & \alpha_{2,n}\\
\vdots & \vdots & \ddots & \vdots\\
\alpha_{n,1} & \alpha_{n,2} & \cdots & \alpha_{n,n}
\end{vmatrix}
}
\]</span></p>

<p>as claimed, QED.</p>

</body>
</html>

